use std::{collections::HashMap, pin::Pin};

use derive_builder::Builder;

use async_openai::{
    config::OpenAIConfig,
    error::OpenAIError,
    types::{
        ChatChoiceLogprobs, ChatCompletionMessageToolCall, ChatCompletionMessageToolCallChunk, ChatCompletionRequestMessage, ChatCompletionResponseFormat
    },
    Client,
};
use serde::{Deserialize, Serialize};
use tokio_stream::Stream;

use crate::services::llm::providers::types::{
    ChatCompletionRequestCommon, CompletionUsage, Role, Stop
};

const OPENROUTER_CHAT_PATH: &str = "/chat/completions";

#[derive(Clone, Serialize, Default, Debug, Builder, Deserialize, PartialEq)]
#[builder(name = "OpenrouterChatCompletionRequestBuilder")]
#[builder(pattern = "mutable")]
#[builder(setter(into, strip_option), default)]
#[builder(derive(Debug))]
pub struct OpenrouterChatCompletionRequest {
    /// Common fields shared across different LLM providers
    #[serde(flatten)]
    pub common: ChatCompletionRequestCommon,

    pub messages: Vec<ChatCompletionRequestMessage>, // min: 1

    #[serde(skip_serializing_if = "Option::is_none")]
    pub frequency_penalty: Option<f32>, // min: -2.0, max: 2.0, default: 0

    #[serde(skip_serializing_if = "Option::is_none")]
    pub logit_bias: Option<HashMap<String, serde_json::Value>>, // default: null

    #[serde(skip_serializing_if = "Option::is_none")]
    pub logprobs: Option<bool>,

    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_logprobs: Option<u8>,

    #[serde(skip_serializing_if = "Option::is_none")]
    pub presence_penalty: Option<f32>, // min: -2.0, max: 2.0, default 0

    #[serde(skip_serializing_if = "Option::is_none")]
    pub response_format: Option<ChatCompletionResponseFormat>,

    #[serde(skip_serializing_if = "Option::is_none")]
    pub seed: Option<i64>,

    #[serde(skip_serializing_if = "Option::is_none")]
    pub stop: Option<Stop>,

    #[serde(skip_serializing_if = "Option::is_none")]
    pub include_reasoning: Option<bool>,
}

/// A chat completion message generated by the model.
#[derive(Debug, Deserialize, Serialize, Clone, PartialEq)]
pub struct OpenrouterChatCompletionResponseMessage {
    /// The contents of the message.
    pub content: Option<String>,

    /// The reasoning part of the message.
    pub reasoning: Option<String>,

    /// The tool calls generated by the model, such as function calls.
    pub tool_calls: Option<Vec<ChatCompletionMessageToolCall>>,

    /// The role of the author of this message.
    pub role: Role,
}

#[derive(Debug, Deserialize, Serialize, Clone, PartialEq)]
pub struct ChatChoice {
    pub index: u32,
    pub message: OpenrouterChatCompletionResponseMessage,
    pub finish_reason: Option<String>,
    pub logprobs: Option<ChatChoiceLogprobs>,
}

#[derive(Debug, Deserialize, Serialize, Clone, PartialEq)]
pub struct ChatChoiceStream {
    pub index: u32,
    pub delta: OpenrouterChatCompletionStreamResponseDelta,
    pub finish_reason: Option<String>,
    pub logprobs: Option<ChatChoiceLogprobs>,
}

/// A chat completion delta generated by streamed model responses.
#[derive(Debug, Deserialize, Serialize, Clone, PartialEq)]
pub struct OpenrouterChatCompletionStreamResponseDelta {
    /// The contents of the chunk message.
    pub content: Option<String>,
    /// The reasoning part of the message.
    pub reasoning: Option<String>,
    pub tool_calls: Option<Vec<ChatCompletionMessageToolCallChunk>>,
    /// The role of the author of this message.
    pub role: Option<Role>,
}

#[derive(Debug, Deserialize, Clone, PartialEq, Serialize)]
pub struct OpenrouterChatCompletionResponse {
    pub id: String,
    pub choices: Vec<ChatChoice>,
    pub created: u32,
    pub model: String,
    pub system_fingerprint: Option<String>,
    pub object: String,
    pub usage: Option<CompletionUsage>,
}

#[derive(Debug, Deserialize, Clone, PartialEq, Serialize)]
pub struct OpenrouterChatCompletionStreamResponse {
    pub id: String,
    pub choices: Vec<ChatChoiceStream>,
    pub created: u32,
    pub model: String,
    pub system_fingerprint: Option<String>,
    pub object: String,
    pub usage: Option<CompletionUsage>,
}

pub type OpenrouterChatCompletionResponseStream =
    Pin<Box<dyn Stream<Item = Result<OpenrouterChatCompletionStreamResponse, OpenAIError>> + Send>>;

/// Encapsulation of OpenRouter's chat API
pub struct OpenrouterChat<'c> {
    client: &'c Client<OpenAIConfig>,
}

impl<'c> OpenrouterChat<'c> {
    pub fn new(client: &'c Client<OpenAIConfig>) -> Self {
        Self { client }
    }

    /// Creates a model response for the given chat conversation.
    pub async fn create(
        &self,
        request: OpenrouterChatCompletionRequest,
    ) -> Result<OpenrouterChatCompletionResponse, OpenAIError> {
        if request.common.stream.is_some() && request.common.stream.unwrap() {
            return Err(OpenAIError::InvalidArgument(
                "When stream is true, use OllamaChat::create_stream".into(),
            ));
        }
        self.client.post(OPENROUTER_CHAT_PATH, request).await
    }

    /// Creates a completion for the chat message
    pub async fn create_stream(
        &self,
        mut request: OpenrouterChatCompletionRequest,
    ) -> Result<OpenrouterChatCompletionResponseStream, OpenAIError> {
        if request.common.stream.is_some() && !request.common.stream.unwrap() {
            return Err(OpenAIError::InvalidArgument(
                "When stream is false, use Chat::create".into(),
            ));
        }

        request.common.stream = Some(true);

        Ok(self.client.post_stream(OPENROUTER_CHAT_PATH, request).await)
    }
}
