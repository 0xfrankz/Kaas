# ü§ñConfigurations et options prises en charge

[![en-icon]](./options.md)
[![zh-hans-icon]](./options_zh-Hans.md)
[![fr-icon]](./options_fr.md)

**Symboles :** ‚úÖ - Pris en charge, ‚ùå - Non pris en charge, üìå - Pr√©vu

## OpenAI ‚úÖ
### Configurations de l'API

| Champ | Description |
| -------- | -------- |
| Cl√© API | La cl√© API pour votre API OpenAI. |
| Mod√®le | ID du mod√®le √† utiliser. |

### Conversation options
### Options de conversation

| Option | Description | Supported |
| - | - | - |
| frequency_penalty | Nombre entre -2.0 et 2.0. Les valeurs positives p√©nalisent les nouveaux jetons en fonction de leur fr√©quence existante dans le texte jusqu'√† pr√©sent, diminuant la probabilit√© que le mod√®le r√©p√®te la m√™me ligne textuellement. | ‚úÖ |
| max_tokens | Le nombre maximum de jetons qui peuvent √™tre g√©n√©r√©s dans l'ach√®vement de la conversation.<br/>La longueur totale des jetons d'entr√©e et des jetons g√©n√©r√©s est limit√©e par la longueur du contexte du mod√®le. | ‚úÖ |
| presence_penalty | Nombre entre -2.0 et 2.0. Les valeurs positives p√©nalisent les nouveaux jetons en fonction de leur pr√©sence dans le texte jusqu'√† pr√©sent, augmentant la probabilit√© que le mod√®le aborde de nouveaux sujets. | ‚úÖ |
| temperature | Quelle temp√©rature d'√©chantillonnage utiliser, entre 0 et 2. Des valeurs plus √©lev√©es comme 0.8 rendront la sortie plus al√©atoire, tandis que des valeurs plus basses comme 0.2 la rendront plus cibl√©e et d√©terministe.<br/>Nous recommandons g√©n√©ralement de modifier cela ou top_p, mais pas les deux. | ‚úÖ |
| top_p | Une alternative √† l'√©chantillonnage avec la temp√©rature, appel√©e √©chantillonnage nucl√©aire, o√π le mod√®le consid√®re les r√©sultats des jetons avec une masse de probabilit√© top_p. Ainsi, 0.1 signifie que seuls les jetons comprenant la masse de probabilit√© sup√©rieure √† 10 % sont consid√©r√©s.<br/>Nous recommandons g√©n√©ralement de modifier cela ou la temp√©rature, mais pas les deux. | ‚úÖ |
| stream | Si d√©fini, des deltas de message partiels seront envoy√©s, comme dans ChatGPT. | ‚úÖ |
| user | Un identifiant unique repr√©sentant votre utilisateur final, ce qui peut aider OpenAI √† surveiller et √† d√©tecter les abus. | ‚úÖ |
| response_format | Un objet sp√©cifiant le format que le mod√®le doit produire. Compatible avec GPT-4 Turbo et tous les mod√®les GPT-3.5 Turbo plus r√©cents que gpt-3.5-turbo-1106. | üìå |
| seed | Si sp√©cifi√©, notre syst√®me fera de son mieux pour √©chantillonner de mani√®re d√©terministe, de sorte que les demandes r√©p√©t√©es avec la m√™me seed et les m√™mes param√®tres devraient renvoyer le m√™me r√©sultat. | üìå |
| stop | Jusqu'√† 4 s√©quences o√π l'API cessera de g√©n√©rer d'autres jetons. | üìå |
| tools | Une liste d'outils que le mod√®le peut appeler. Actuellement, seules les fonctions sont prises en charge en tant qu'outil. Utilisez ceci pour fournir une liste de fonctions pour lesquelles le mod√®le peut g√©n√©rer des entr√©es JSON. | ‚ùå |
| tool_choice | Contr√¥le quelle fonction (le cas √©ch√©ant) est appel√©e par le mod√®le. none signifie que le mod√®le n'appellera pas de fonction et g√©n√©rera plut√¥t un message. auto signifie que le mod√®le peut choisir entre g√©n√©rer un message ou appeler une fonction. Sp√©cifier une fonction particuli√®re via {"type": "function", "function": {"name": "my_function"}} force le mod√®le √† appeler cette fonction.<br/>none est la valeur par d√©faut lorsqu'aucune fonction n'est pr√©sente. auto est la valeur par d√©faut si des fonctions sont pr√©sentes. | ‚ùå |
| logit_bias | Modifie la probabilit√© que des jetons sp√©cifi√©s apparaissent dans l'ach√®vement. <br/> Accepte un objet JSON qui mappe les jetons (sp√©cifi√©s par leur ID de jeton dans le tokenizer) √† une valeur de biais associ√©e de -100 √† 100. Math√©matiquement, le biais est ajout√© aux logits g√©n√©r√©s par le mod√®le avant l'√©chantillonnage. L'effet exact variera selon le mod√®le, mais les valeurs entre -1 et 1 devraient diminuer ou augmenter la probabilit√© de s√©lection¬†; les valeurs comme -100 ou 100 devraient entra√Æner une interdiction ou une s√©lection exclusive du jeton pertinent. | ‚ùå |
| logprobs | Indique s'il faut renvoyer ou non les probabilit√©s logarithmiques des jetons de sortie. Si la valeur est true, renvoie les probabilit√©s logarithmiques de chaque jeton de sortie renvoy√© dans le contenu du message. Cette option n'est actuellement pas disponible sur le mod√®le gpt-4-vision-preview. | ‚ùå |
| top_logprobs | Un entier entre 0 et 5 sp√©cifiant le nombre de jetons les plus probables √† renvoyer √† chaque position de jeton, chacun avec une probabilit√© logarithmique associ√©e. logprobs doit √™tre d√©fini sur true si ce param√®tre est utilis√©. | ‚ùå |
| n | Combien de choix d'ach√®vement de conversation g√©n√©rer pour chaque message d'entr√©e. Notez que vous serez factur√© en fonction du nombre de jetons g√©n√©r√©s dans tous les choix. Gardez n √† 1 pour minimiser les co√ªts. | ‚ùå |


### References

- [OpenAI Documentation](https://platform.openai.com/docs/guides/text-generation/chat-completions-api)

## Microsoft Azure ‚úÖ
## Microsoft Azure ‚úÖ

### Configurations de l'API

| Champ | Description |
| -------- | -------- |
| Cl√© API | La cl√© API pour votre API Azure OpenAI. |
| Point de terminaison | Le point de terminaison pour votre API Azure OpenAI. |
| Version de l'API | La version de l'API √† utiliser pour cette op√©ration. Ceci suit le format AAAA-MM-JJ ou AAAA-MM-JJ-preview. |
| ID de d√©ploiement | Le nom du d√©ploiement de votre mod√®le. |

### Options de conversation
| Option | Description | Pris en charge |
| - | - | - |
| max_tokens | Le nombre maximum de jetons √† g√©n√©rer dans l'ach√®vement. Le nombre de jetons de votre prompt plus max_tokens ne peut pas d√©passer la longueur de contexte du mod√®le. | ‚úÖ |
| temperature | Quelle temp√©rature d'√©chantillonnage utiliser, entre 0 et 2. Des valeurs plus √©lev√©es signifient que le mod√®le prend plus de risques. Essayez 0.9 pour des applications plus cr√©atives, et 0 (√©chantillonnage argmax) pour celles avec une r√©ponse bien d√©finie. Nous recommandons g√©n√©ralement de modifier ceci ou top_p mais pas les deux. | ‚úÖ |
| top_p | Une alternative √† l'√©chantillonnage avec temp√©rature, appel√©e √©chantillonnage nucleus, o√π le mod√®le consid√®re les r√©sultats des jetons avec une masse de probabilit√© top_p. Ainsi, 0.1 signifie que seuls les jetons comprenant les 10% sup√©rieurs de la masse de probabilit√© sont consid√©r√©s. Nous recommandons g√©n√©ralement de modifier ceci ou la temp√©rature mais pas les deux. | ‚úÖ |
| presence_penalty | Nombre entre -2.0 et 2.0. Les valeurs positives p√©nalisent les nouveaux jetons en fonction de leur pr√©sence dans le texte jusqu'√† pr√©sent, augmentant la probabilit√© que le mod√®le aborde de nouveaux sujets. | ‚úÖ |
| frequency_penalty | Nombre entre -2.0 et 2.0. Les valeurs positives p√©nalisent les nouveaux jetons en fonction de leur fr√©quence existante dans le texte jusqu'√† pr√©sent, diminuant la probabilit√© que le mod√®le r√©p√®te la m√™me ligne textuellement. | ‚úÖ |
| stream | Si d√©fini, des deltas de message partiels seront envoy√©s, comme dans ChatGPT. | ‚úÖ |
| user | Un identifiant unique repr√©sentant votre utilisateur final, ce qui peut aider OpenAI √† surveiller et √† d√©tecter les abus. | ‚úÖ |
| suffix | Le suffixe qui vient apr√®s un ach√®vement du texte ins√©r√©. | üìå |
| echo | Renvoie le prompt en plus de l'ach√®vement. Ce param√®tre ne peut pas √™tre utilis√© avec gpt-35-turbo. | üìå |
| stop | Jusqu'√† quatre s√©quences o√π l'API arr√™tera de g√©n√©rer d'autres jetons. Le texte renvoy√© ne contiendra pas la s√©quence d'arr√™t. Pour GPT-4 Turbo avec Vision, jusqu'√† deux s√©quences sont prises en charge. | üìå |
| logit_bias | Modifie la probabilit√© que des jetons sp√©cifi√©s apparaissent dans l'ach√®vement. Accepte un objet json qui mappe les jetons (sp√©cifi√©s par leur ID de jeton dans le tokenizer GPT) √† une valeur de biais associ√©e de -100 √† 100. Math√©matiquement, le biais est ajout√© aux logits g√©n√©r√©s par le mod√®le avant l'√©chantillonnage. L'effet exact varie selon le mod√®le, mais les valeurs entre -1 et 1 devraient diminuer ou augmenter la probabilit√© de s√©lection ; les valeurs comme -100 ou 100 devraient entra√Æner une interdiction ou une s√©lection exclusive du jeton pertinent. | ‚ùå |
| n | Combien de choix d'ach√®vement de conversation g√©n√©rer pour chaque message d'entr√©e. Notez que vous serez factur√© en fonction du nombre de jetons g√©n√©r√©s dans tous les choix. Gardez n √† 1 pour minimiser les co√ªts. | ‚ùå |
| logprobs | Inclut les probabilit√©s logarithmiques sur les jetons les plus probables de logprobs, ainsi que les jetons choisis. Ce param√®tre ne peut pas √™tre utilis√© avec gpt-35-turbo. | ‚ùå |
| best_of | G√©n√®re best_of ach√®vements c√¥t√© serveur et renvoie le "meilleur" (celui avec la plus faible probabilit√© logarithmique par jeton). Les r√©sultats ne peuvent pas √™tre diffus√©s en continu. Lorsqu'il est utilis√© avec n, best_of contr√¥le le nombre d'ach√®vements candidats et n sp√©cifie combien en renvoyer ‚Äì best_of doit √™tre sup√©rieur √† n. Ce param√®tre ne peut pas √™tre utilis√© avec gpt-35-turbo. | ‚ùå |

### R√©f√©rences

- [Documentation Azure](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions)

## Anthropic Claude ‚úÖ

### Configurations de l'API

| Champ | Description |
| -------- | -------- |
| cl√©-api | La cl√© API pour votre API Anthropic. |
| version-anthropic | La version d'Anthropic √† utiliser. |
| mod√®le | Le mod√®le Anthropic √† utiliser. |

### Options de conversation

| Option | Description | Pris en charge |
| - | - | - |
| max_tokens | Le nombre maximum de jetons √† g√©n√©rer avant l'arr√™t. | ‚úÖ |
| temperature | Quantit√© d'al√©atoire inject√©e dans la r√©ponse.<br/>Par d√©faut √† 1.0. Varie de 0.0 √† 1.0. Utilisez une temp√©rature proche de 0.0 pour l'analytique / choix multiple, et proche de 1.0 pour les t√¢ches cr√©atives et g√©n√©ratives.<br/>Nous recommandons g√©n√©ralement de modifier ceci ou top_p mais pas les deux. | ‚úÖ |
| top_p | Utilise l'√©chantillonnage du noyau.<br/>Recommand√© uniquement pour les cas d'utilisation avanc√©s. Vous n'avez g√©n√©ralement besoin d'utiliser que _temperature_. | ‚úÖ |
| stream | Si la r√©ponse doit √™tre diffus√©e de mani√®re incr√©mentielle en utilisant les √©v√©nements envoy√©s par le serveur. | ‚úÖ |
| user | Un objet d√©crivant les m√©tadonn√©es de la requ√™te. <br/>_metadata.user_id_: Un identifiant externe pour l'utilisateur associ√© √† la requ√™te. | ‚úÖ |
| stop_sequences | S√©quences de texte personnalis√©es qui feront arr√™ter la g√©n√©ration du mod√®le. | üìå |
| top_k | √âchantillonne uniquement parmi les K meilleures options pour chaque jeton suivant.<br/>Recommand√© uniquement pour les cas d'utilisation avanc√©s. Vous n'avez g√©n√©ralement besoin d'utiliser que _temperature_. | üìå |
| tools | D√©finitions des outils que le mod√®le peut utiliser. | ‚ùå |
| tool_choice | Comment le mod√®le doit utiliser les outils fournis. | ‚ùå |

## Ollama ‚úÖ
### Configurations de l'API

| Champ | Description |
| - | - |
| Point de terminaison | Le point de terminaison pour votre API Azure OpenAI. |
| Mod√®le | Le mod√®le √† utiliser. |

### Options de conversation

| Option | Description | Pris en charge |
| - | - | - |
| num_ctx | Nombre de jetons d'entr√©e. D√©finit la taille de la fen√™tre de contexte utilis√©e pour g√©n√©rer le jeton suivant. (Par d√©faut : 2048) | ‚úÖ |
| num-predict | Nombre de jetons de sortie. Nombre maximum de jetons √† pr√©dire lors de la g√©n√©ration de texte. (Par d√©faut : 128, -1 = g√©n√©ration infinie, -2 = remplir le contexte) | ‚úÖ |
| temperature | La temp√©rature du mod√®le. Augmenter la temp√©rature rendra les r√©ponses du mod√®le plus cr√©atives. (Par d√©faut : 0.8) | ‚úÖ |
| top_p | Fonctionne avec top-k. Une valeur plus √©lev√©e (par ex., 0.95) conduira √† un texte plus diversifi√©, tandis qu'une valeur plus basse (par ex., 0.5) g√©n√©rera un texte plus cibl√© et conservateur. (Par d√©faut : 0.9) | ‚úÖ |
| mirostat | Active l'√©chantillonnage Mirostat pour contr√¥ler la perplexit√©. (par d√©faut : 0, 0 = d√©sactiv√©, 1 = Mirostat, 2 = Mirostat 2.0) | üìå |
| mirostat_eta | Influence la rapidit√© de r√©action de l'algorithme au feedback du texte g√©n√©r√©. Un taux d'apprentissage plus bas entra√Ænera des ajustements plus lents, tandis qu'un taux plus √©lev√© rendra l'algorithme plus r√©actif. (Par d√©faut : 0.1) | üìå |
| mirostat_tau | Contr√¥le l'√©quilibre entre coh√©rence et diversit√© de la sortie. Une valeur plus basse donnera un texte plus cibl√© et coh√©rent. (Par d√©faut : 5.0) | üìå |
| repeat_last_n | D√©finit jusqu'o√π le mod√®le doit regarder en arri√®re pour √©viter la r√©p√©tition. (Par d√©faut : 64, 0 = d√©sactiv√©, -1 = num_ctx) | üìå |
| repeat_penalty | D√©finit l'intensit√© de la p√©nalit√© pour les r√©p√©titions. Une valeur plus √©lev√©e (par ex., 1.5) p√©nalisera plus fortement les r√©p√©titions, tandis qu'une valeur plus basse (par ex., 0.9) sera plus tol√©rante. (Par d√©faut : 1.1) | üìå |
| seed | D√©finit la graine al√©atoire √† utiliser pour la g√©n√©ration. La d√©finition d'un nombre sp√©cifique fera g√©n√©rer le m√™me texte pour le m√™me prompt. (Par d√©faut : 0) | üìå |
| stop | D√©finit les s√©quences d'arr√™t √† utiliser. Lorsque ce motif est rencontr√©, le LLM arr√™tera de g√©n√©rer du texte et retournera. Plusieurs motifs d'arr√™t peuvent √™tre d√©finis en sp√©cifiant plusieurs param√®tres stop s√©par√©s dans un modelfile. | üìå |
| tfs_z | L'√©chantillonnage sans queue est utilis√© pour r√©duire l'impact des jetons moins probables de la sortie. Une valeur plus √©lev√©e (par ex., 2.0) r√©duira davantage l'impact, tandis qu'une valeur de 1.0 d√©sactive ce param√®tre. (par d√©faut : 1) | üìå |
| top_k | R√©duit la probabilit√© de g√©n√©rer du non-sens. Une valeur plus √©lev√©e (par ex., 100) donnera des r√©ponses plus diverses, tandis qu'une valeur plus basse (par ex., 10) sera plus conservatrice. (Par d√©faut : 40) | üìå |
| min_p | Alternative au top_p, vise √† assurer un √©quilibre entre qualit√© et vari√©t√©. Le param√®tre p repr√©sente la probabilit√© minimale pour qu'un jeton soit consid√©r√©, par rapport √† la probabilit√© du jeton le plus probable. Par exemple, avec p=0.05 et le jeton le plus probable ayant une probabilit√© de 0.9, les logits avec une valeur inf√©rieure √† 0.045 sont filtr√©s. (Par d√©faut : 0.0) | üìå |

### R√©f√©rences
- [Ollama Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values)

## Google Gemini

üìå **Support Planifi√©**

[en-icon]: https://img.shields.io/badge/English-teal?style=flat-square
[zh-hans-icon]: https://img.shields.io/badge/%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-teal?style=flat-square
[fr-icon]: https://img.shields.io/badge/Fran√ßais-teal?style=flat-square