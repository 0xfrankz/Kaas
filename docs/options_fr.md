# ğŸ¤–Configurations et options prises en charge

[![en-icon]](./options.md)
[![zh-hans-icon]](./options_zh-Hans.md)
[![fr-icon]](./options_fr.md)

**Symboles :** âœ… - Pris en charge, âŒ - Non pris en charge, ğŸ“Œ - PrÃ©vu

## OpenAI âœ…
### Configurations de l'API

| Champ | Description |
| -------- | -------- |
| ClÃ© API | La clÃ© API pour votre API OpenAI. |
| ModÃ¨le | ID du modÃ¨le Ã  utiliser. |

### Conversation options
### Options de conversation

| Option | Description | Supported |
| - | - | - |
| frequency_penalty | Nombre entre -2.0 et 2.0. Les valeurs positives pÃ©nalisent les nouveaux jetons en fonction de leur frÃ©quence existante dans le texte jusqu'Ã  prÃ©sent, diminuant la probabilitÃ© que le modÃ¨le rÃ©pÃ¨te la mÃªme ligne textuellement. | âœ… |
| max_tokens | Le nombre maximum de jetons qui peuvent Ãªtre gÃ©nÃ©rÃ©s dans l'achÃ¨vement de la conversation.<br/>La longueur totale des jetons d'entrÃ©e et des jetons gÃ©nÃ©rÃ©s est limitÃ©e par la longueur du contexte du modÃ¨le. | âœ… |
| presence_penalty | Nombre entre -2.0 et 2.0. Les valeurs positives pÃ©nalisent les nouveaux jetons en fonction de leur prÃ©sence dans le texte jusqu'Ã  prÃ©sent, augmentant la probabilitÃ© que le modÃ¨le aborde de nouveaux sujets. | âœ… |
| temperature | Quelle tempÃ©rature d'Ã©chantillonnage utiliser, entre 0 et 2. Des valeurs plus Ã©levÃ©es comme 0.8 rendront la sortie plus alÃ©atoire, tandis que des valeurs plus basses comme 0.2 la rendront plus ciblÃ©e et dÃ©terministe.<br/>Nous recommandons gÃ©nÃ©ralement de modifier cela ou top_p, mais pas les deux. | âœ… |
| top_p | Une alternative Ã  l'Ã©chantillonnage avec la tempÃ©rature, appelÃ©e Ã©chantillonnage nuclÃ©aire, oÃ¹ le modÃ¨le considÃ¨re les rÃ©sultats des jetons avec une masse de probabilitÃ© top_p. Ainsi, 0.1 signifie que seuls les jetons comprenant la masse de probabilitÃ© supÃ©rieure Ã  10 % sont considÃ©rÃ©s.<br/>Nous recommandons gÃ©nÃ©ralement de modifier cela ou la tempÃ©rature, mais pas les deux. | âœ… |
| stream | Si dÃ©fini, des deltas de message partiels seront envoyÃ©s, comme dans ChatGPT. | âœ… |
| user | Un identifiant unique reprÃ©sentant votre utilisateur final, ce qui peut aider OpenAI Ã  surveiller et Ã  dÃ©tecter les abus. | âœ… |
| response_format | Un objet spÃ©cifiant le format que le modÃ¨le doit produire. Compatible avec GPT-4 Turbo et tous les modÃ¨les GPT-3.5 Turbo plus rÃ©cents que gpt-3.5-turbo-1106. | ğŸ“Œ |
| seed | Si spÃ©cifiÃ©, notre systÃ¨me fera de son mieux pour Ã©chantillonner de maniÃ¨re dÃ©terministe, de sorte que les demandes rÃ©pÃ©tÃ©es avec la mÃªme seed et les mÃªmes paramÃ¨tres devraient renvoyer le mÃªme rÃ©sultat. | ğŸ“Œ |
| stop | Jusqu'Ã  4 sÃ©quences oÃ¹ l'API cessera de gÃ©nÃ©rer d'autres jetons. | ğŸ“Œ |
| tools | Une liste d'outils que le modÃ¨le peut appeler. Actuellement, seules les fonctions sont prises en charge en tant qu'outil. Utilisez ceci pour fournir une liste de fonctions pour lesquelles le modÃ¨le peut gÃ©nÃ©rer des entrÃ©es JSON. | âŒ |
| tool_choice | ContrÃ´le quelle fonction (le cas Ã©chÃ©ant) est appelÃ©e par le modÃ¨le. none signifie que le modÃ¨le n'appellera pas de fonction et gÃ©nÃ©rera plutÃ´t un message. auto signifie que le modÃ¨le peut choisir entre gÃ©nÃ©rer un message ou appeler une fonction. SpÃ©cifier une fonction particuliÃ¨re via {"type": "function", "function": {"name": "my_function"}} force le modÃ¨le Ã  appeler cette fonction.<br/>none est la valeur par dÃ©faut lorsqu'aucune fonction n'est prÃ©sente. auto est la valeur par dÃ©faut si des fonctions sont prÃ©sentes. | âŒ |
| logit_bias | Modifie la probabilitÃ© que des jetons spÃ©cifiÃ©s apparaissent dans l'achÃ¨vement. <br/> Accepte un objet JSON qui mappe les jetons (spÃ©cifiÃ©s par leur ID de jeton dans le tokenizer) Ã  une valeur de biais associÃ©e de -100 Ã  100. MathÃ©matiquement, le biais est ajoutÃ© aux logits gÃ©nÃ©rÃ©s par le modÃ¨le avant l'Ã©chantillonnage. L'effet exact variera selon le modÃ¨le, mais les valeurs entre -1 et 1 devraient diminuer ou augmenter la probabilitÃ© de sÃ©lectionÂ ; les valeurs comme -100 ou 100 devraient entraÃ®ner une interdiction ou une sÃ©lection exclusive du jeton pertinent. | âŒ |
| logprobs | Indique s'il faut renvoyer ou non les probabilitÃ©s logarithmiques des jetons de sortie. Si la valeur est true, renvoie les probabilitÃ©s logarithmiques de chaque jeton de sortie renvoyÃ© dans le contenu du message. Cette option n'est actuellement pas disponible sur le modÃ¨le gpt-4-vision-preview. | âŒ |
| top_logprobs | Un entier entre 0 et 5 spÃ©cifiant le nombre de jetons les plus probables Ã  renvoyer Ã  chaque position de jeton, chacun avec une probabilitÃ© logarithmique associÃ©e. logprobs doit Ãªtre dÃ©fini sur true si ce paramÃ¨tre est utilisÃ©. | âŒ |
| n | Combien de choix d'achÃ¨vement de conversation gÃ©nÃ©rer pour chaque message d'entrÃ©e. Notez que vous serez facturÃ© en fonction du nombre de jetons gÃ©nÃ©rÃ©s dans tous les choix. Gardez n Ã  1 pour minimiser les coÃ»ts. | âŒ |


### References

- [OpenAI Documentation](https://platform.openai.com/docs/guides/text-generation/chat-completions-api)

## Microsoft Azure âœ…
## Microsoft Azure âœ…

### Configurations de l'API

| Champ | Description |
| -------- | -------- |
| ClÃ© API | La clÃ© API pour votre API Azure OpenAI. |
| Point de terminaison | Le point de terminaison pour votre API Azure OpenAI. |
| Version de l'API | La version de l'API Ã  utiliser pour cette opÃ©ration. Ceci suit le format AAAA-MM-JJ ou AAAA-MM-JJ-preview. |
| ID de dÃ©ploiement | Le nom du dÃ©ploiement de votre modÃ¨le. |

### Options de conversation
| Option | Description | Pris en charge |
| - | - | - |
| max_tokens | Le nombre maximum de jetons Ã  gÃ©nÃ©rer dans l'achÃ¨vement. Le nombre de jetons de votre prompt plus max_tokens ne peut pas dÃ©passer la longueur de contexte du modÃ¨le. | âœ… |
| temperature | Quelle tempÃ©rature d'Ã©chantillonnage utiliser, entre 0 et 2. Des valeurs plus Ã©levÃ©es signifient que le modÃ¨le prend plus de risques. Essayez 0.9 pour des applications plus crÃ©atives, et 0 (Ã©chantillonnage argmax) pour celles avec une rÃ©ponse bien dÃ©finie. Nous recommandons gÃ©nÃ©ralement de modifier ceci ou top_p mais pas les deux. | âœ… |
| top_p | Une alternative Ã  l'Ã©chantillonnage avec tempÃ©rature, appelÃ©e Ã©chantillonnage nucleus, oÃ¹ le modÃ¨le considÃ¨re les rÃ©sultats des jetons avec une masse de probabilitÃ© top_p. Ainsi, 0.1 signifie que seuls les jetons comprenant les 10% supÃ©rieurs de la masse de probabilitÃ© sont considÃ©rÃ©s. Nous recommandons gÃ©nÃ©ralement de modifier ceci ou la tempÃ©rature mais pas les deux. | âœ… |
| presence_penalty | Nombre entre -2.0 et 2.0. Les valeurs positives pÃ©nalisent les nouveaux jetons en fonction de leur prÃ©sence dans le texte jusqu'Ã  prÃ©sent, augmentant la probabilitÃ© que le modÃ¨le aborde de nouveaux sujets. | âœ… |
| frequency_penalty | Nombre entre -2.0 et 2.0. Les valeurs positives pÃ©nalisent les nouveaux jetons en fonction de leur frÃ©quence existante dans le texte jusqu'Ã  prÃ©sent, diminuant la probabilitÃ© que le modÃ¨le rÃ©pÃ¨te la mÃªme ligne textuellement. | âœ… |
| stream | Si dÃ©fini, des deltas de message partiels seront envoyÃ©s, comme dans ChatGPT. | âœ… |
| user | Un identifiant unique reprÃ©sentant votre utilisateur final, ce qui peut aider OpenAI Ã  surveiller et Ã  dÃ©tecter les abus. | âœ… |
| suffix | Le suffixe qui vient aprÃ¨s un achÃ¨vement du texte insÃ©rÃ©. | ğŸ“Œ |
| echo | Renvoie le prompt en plus de l'achÃ¨vement. Ce paramÃ¨tre ne peut pas Ãªtre utilisÃ© avec gpt-35-turbo. | ğŸ“Œ |
| stop | Jusqu'Ã  quatre sÃ©quences oÃ¹ l'API arrÃªtera de gÃ©nÃ©rer d'autres jetons. Le texte renvoyÃ© ne contiendra pas la sÃ©quence d'arrÃªt. Pour GPT-4 Turbo avec Vision, jusqu'Ã  deux sÃ©quences sont prises en charge. | ğŸ“Œ |
| logit_bias | Modifie la probabilitÃ© que des jetons spÃ©cifiÃ©s apparaissent dans l'achÃ¨vement. Accepte un objet json qui mappe les jetons (spÃ©cifiÃ©s par leur ID de jeton dans le tokenizer GPT) Ã  une valeur de biais associÃ©e de -100 Ã  100. MathÃ©matiquement, le biais est ajoutÃ© aux logits gÃ©nÃ©rÃ©s par le modÃ¨le avant l'Ã©chantillonnage. L'effet exact varie selon le modÃ¨le, mais les valeurs entre -1 et 1 devraient diminuer ou augmenter la probabilitÃ© de sÃ©lection ; les valeurs comme -100 ou 100 devraient entraÃ®ner une interdiction ou une sÃ©lection exclusive du jeton pertinent. | âŒ |
| n | Combien de choix d'achÃ¨vement de conversation gÃ©nÃ©rer pour chaque message d'entrÃ©e. Notez que vous serez facturÃ© en fonction du nombre de jetons gÃ©nÃ©rÃ©s dans tous les choix. Gardez n Ã  1 pour minimiser les coÃ»ts. | âŒ |
| logprobs | Inclut les probabilitÃ©s logarithmiques sur les jetons les plus probables de logprobs, ainsi que les jetons choisis. Ce paramÃ¨tre ne peut pas Ãªtre utilisÃ© avec gpt-35-turbo. | âŒ |
| best_of | GÃ©nÃ¨re best_of achÃ¨vements cÃ´tÃ© serveur et renvoie le "meilleur" (celui avec la plus faible probabilitÃ© logarithmique par jeton). Les rÃ©sultats ne peuvent pas Ãªtre diffusÃ©s en continu. Lorsqu'il est utilisÃ© avec n, best_of contrÃ´le le nombre d'achÃ¨vements candidats et n spÃ©cifie combien en renvoyer â€“ best_of doit Ãªtre supÃ©rieur Ã  n. Ce paramÃ¨tre ne peut pas Ãªtre utilisÃ© avec gpt-35-turbo. | âŒ |

### RÃ©fÃ©rences

- [Documentation Azure](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions)

## Anthropic Claude âœ…

### Configurations de l'API

| Champ | Description |
| -------- | -------- |
| clÃ©-api | La clÃ© API pour votre API Anthropic. |
| version-anthropic | La version d'Anthropic Ã  utiliser. |
| modÃ¨le | Le modÃ¨le Anthropic Ã  utiliser. |

### Options de conversation

| Option | Description | Pris en charge |
| - | - | - |
| max_tokens | Le nombre maximum de jetons Ã  gÃ©nÃ©rer avant l'arrÃªt. | âœ… |
| temperature | QuantitÃ© d'alÃ©atoire injectÃ©e dans la rÃ©ponse.<br/>Par dÃ©faut Ã  1.0. Varie de 0.0 Ã  1.0. Utilisez une tempÃ©rature proche de 0.0 pour l'analytique / choix multiple, et proche de 1.0 pour les tÃ¢ches crÃ©atives et gÃ©nÃ©ratives.<br/>Nous recommandons gÃ©nÃ©ralement de modifier ceci ou top_p mais pas les deux. | âœ… |
| top_p | Utilise l'Ã©chantillonnage du noyau.<br/>RecommandÃ© uniquement pour les cas d'utilisation avancÃ©s. Vous n'avez gÃ©nÃ©ralement besoin d'utiliser que _temperature_. | âœ… |
| stream | Si la rÃ©ponse doit Ãªtre diffusÃ©e de maniÃ¨re incrÃ©mentielle en utilisant les Ã©vÃ©nements envoyÃ©s par le serveur. | âœ… |
| user | Un objet dÃ©crivant les mÃ©tadonnÃ©es de la requÃªte. <br/>_metadata.user_id_: Un identifiant externe pour l'utilisateur associÃ© Ã  la requÃªte. | âœ… |
| stop_sequences | SÃ©quences de texte personnalisÃ©es qui feront arrÃªter la gÃ©nÃ©ration du modÃ¨le. | ğŸ“Œ |
| top_k | Ã‰chantillonne uniquement parmi les K meilleures options pour chaque jeton suivant.<br/>RecommandÃ© uniquement pour les cas d'utilisation avancÃ©s. Vous n'avez gÃ©nÃ©ralement besoin d'utiliser que _temperature_. | ğŸ“Œ |
| tools | DÃ©finitions des outils que le modÃ¨le peut utiliser. | âŒ |
| tool_choice | Comment le modÃ¨le doit utiliser les outils fournis. | âŒ |

## Ollama âœ…
### Configurations de l'API

| Champ | Description |
| - | - |
| Point de terminaison | Le point de terminaison pour votre API Azure OpenAI. |
| ModÃ¨le | Le modÃ¨le Ã  utiliser. |

### Options de conversation

| Option | Description | Pris en charge |
| - | - | - |
| num_ctx | Nombre de jetons d'entrÃ©e. DÃ©finit la taille de la fenÃªtre de contexte utilisÃ©e pour gÃ©nÃ©rer le jeton suivant. (Par dÃ©faut : 2048) | âœ… |
| num-predict | Nombre de jetons de sortie. Nombre maximum de jetons Ã  prÃ©dire lors de la gÃ©nÃ©ration de texte. (Par dÃ©faut : 128, -1 = gÃ©nÃ©ration infinie, -2 = remplir le contexte) | âœ… |
| temperature | La tempÃ©rature du modÃ¨le. Augmenter la tempÃ©rature rendra les rÃ©ponses du modÃ¨le plus crÃ©atives. (Par dÃ©faut : 0.8) | âœ… |
| top_p | Fonctionne avec top-k. Une valeur plus Ã©levÃ©e (par ex., 0.95) conduira Ã  un texte plus diversifiÃ©, tandis qu'une valeur plus basse (par ex., 0.5) gÃ©nÃ©rera un texte plus ciblÃ© et conservateur. (Par dÃ©faut : 0.9) | âœ… |
| mirostat | Active l'Ã©chantillonnage Mirostat pour contrÃ´ler la perplexitÃ©. (par dÃ©faut : 0, 0 = dÃ©sactivÃ©, 1 = Mirostat, 2 = Mirostat 2.0) | ğŸ“Œ |
| mirostat_eta | Influence la rapiditÃ© de rÃ©action de l'algorithme au feedback du texte gÃ©nÃ©rÃ©. Un taux d'apprentissage plus bas entraÃ®nera des ajustements plus lents, tandis qu'un taux plus Ã©levÃ© rendra l'algorithme plus rÃ©actif. (Par dÃ©faut : 0.1) | ğŸ“Œ |
| mirostat_tau | ContrÃ´le l'Ã©quilibre entre cohÃ©rence et diversitÃ© de la sortie. Une valeur plus basse donnera un texte plus ciblÃ© et cohÃ©rent. (Par dÃ©faut : 5.0) | ğŸ“Œ |
| repeat_last_n | DÃ©finit jusqu'oÃ¹ le modÃ¨le doit regarder en arriÃ¨re pour Ã©viter la rÃ©pÃ©tition. (Par dÃ©faut : 64, 0 = dÃ©sactivÃ©, -1 = num_ctx) | ğŸ“Œ |
| repeat_penalty | DÃ©finit l'intensitÃ© de la pÃ©nalitÃ© pour les rÃ©pÃ©titions. Une valeur plus Ã©levÃ©e (par ex., 1.5) pÃ©nalisera plus fortement les rÃ©pÃ©titions, tandis qu'une valeur plus basse (par ex., 0.9) sera plus tolÃ©rante. (Par dÃ©faut : 1.1) | ğŸ“Œ |
| seed | DÃ©finit la graine alÃ©atoire Ã  utiliser pour la gÃ©nÃ©ration. La dÃ©finition d'un nombre spÃ©cifique fera gÃ©nÃ©rer le mÃªme texte pour le mÃªme prompt. (Par dÃ©faut : 0) | ğŸ“Œ |
| stop | DÃ©finit les sÃ©quences d'arrÃªt Ã  utiliser. Lorsque ce motif est rencontrÃ©, le LLM arrÃªtera de gÃ©nÃ©rer du texte et retournera. Plusieurs motifs d'arrÃªt peuvent Ãªtre dÃ©finis en spÃ©cifiant plusieurs paramÃ¨tres stop sÃ©parÃ©s dans un modelfile. | ğŸ“Œ |
| tfs_z | L'Ã©chantillonnage sans queue est utilisÃ© pour rÃ©duire l'impact des jetons moins probables de la sortie. Une valeur plus Ã©levÃ©e (par ex., 2.0) rÃ©duira davantage l'impact, tandis qu'une valeur de 1.0 dÃ©sactive ce paramÃ¨tre. (par dÃ©faut : 1) | ğŸ“Œ |
| top_k | RÃ©duit la probabilitÃ© de gÃ©nÃ©rer du non-sens. Une valeur plus Ã©levÃ©e (par ex., 100) donnera des rÃ©ponses plus diverses, tandis qu'une valeur plus basse (par ex., 10) sera plus conservatrice. (Par dÃ©faut : 40) | ğŸ“Œ |
| min_p | Alternative au top_p, vise Ã  assurer un Ã©quilibre entre qualitÃ© et variÃ©tÃ©. Le paramÃ¨tre p reprÃ©sente la probabilitÃ© minimale pour qu'un jeton soit considÃ©rÃ©, par rapport Ã  la probabilitÃ© du jeton le plus probable. Par exemple, avec p=0.05 et le jeton le plus probable ayant une probabilitÃ© de 0.9, les logits avec une valeur infÃ©rieure Ã  0.045 sont filtrÃ©s. (Par dÃ©faut : 0.0) | ğŸ“Œ |

### RÃ©fÃ©rences
- [Ollama Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values)

## Google Gemini

ğŸ“Œ **Support PlanifiÃ©**

[en-icon]: https://img.shields.io/badge/English-teal?style=flat-square
[zh-hans-icon]: https://img.shields.io/badge/%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-teal?style=flat-square
[fr-icon]: https://img.shields.io/badge/FranÃ§ais-teal?style=flat-square